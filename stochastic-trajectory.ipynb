{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c689b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2feca066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    GPT2TimeLMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04d94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 19:39:29.430893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../language-modeling')\n",
    "from run_time_clm import (\n",
    "    get_checkpoint,\n",
    "    get_special_tokens,\n",
    "    get_data_paths,\n",
    "    get_dataset)\n",
    "\n",
    "sys.path.append('../text-generation')\n",
    "from generation_metrics import GenerationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd340ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa75e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2TimeLMHeadModel, GPT2Tokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d75602a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        \n",
    "def adjust_length_to_model(length, max_sequence_length):\n",
    "    if length < 0 and max_sequence_length > 0:\n",
    "        length = max_sequence_length\n",
    "    elif 0 < max_sequence_length < length:\n",
    "        length = max_sequence_length  # No generation bigger than model size\n",
    "    elif length < 0:\n",
    "        length = MAX_LENGTH  # avoid infinite loop\n",
    "    return length\n",
    "\n",
    "def simulate_brownian_bridge(B_0, B_T, num_samples, sentence_lengths, dt=0.05, mu=0.0, sigma=1.0):\n",
    "    \"\"\"Run bridge forward pinned at B_0 and B_T\"\"\"\n",
    "    if isinstance(B_0, torch.Tensor):\n",
    "        B_0 = B_0.cpu().detach().numpy()\n",
    "    if isinstance(B_T, torch.Tensor):\n",
    "        B_T = B_T.cpu().detach().numpy()\n",
    "\n",
    "    bridge = [B_0]\n",
    "    x_t = np.copy(B_0)\n",
    "    for step in range(num_samples - 2): # number of sentences\n",
    "        dim = B_0.shape[-1]\n",
    "        noise = np.sqrt(dt)*sigma*np.random.normal(mu, sigma, dim)\n",
    "        t = step/num_samples\n",
    "        x_tp1 = x_t * (1- dt/(1-t)) + (dt/(1-t))*B_T + noise\n",
    "        length_idx = step % len(sentence_lengths)\n",
    "        bridge += [x_tp1] * sentence_lengths[length_idx]\n",
    "        x_t = x_tp1\n",
    "\n",
    "    length_idx = step % len(sentence_lengths)\n",
    "    bridge += [B_T] * sentence_lengths[length_idx]\n",
    "\n",
    "    return bridge\n",
    "\n",
    "def split_text(raw_text):\n",
    "    split_pattern = \". \"\n",
    "    split_raw_text = [_ + split_pattern for _ in raw_text.split(split_pattern)]\n",
    "    split_raw_text[-1] = split_raw_text[-1].rstrip(split_pattern)\n",
    "    return split_raw_text\n",
    "\n",
    "def get_density(dataset, lm, cl_model):\n",
    "    \"\"\"Estimate density of last latent\"\"\"\n",
    "    first_latents = []\n",
    "    last_latents = []\n",
    "    length = len(dataset)\n",
    "    print(length)\n",
    "    for text_i in range(length):\n",
    "        first_latents.append(dataset.cl_embeddings[text_i][0].detach().cpu().numpy())\n",
    "        last_latents.append(dataset.cl_embeddings[text_i][-1].detach().cpu().numpy())\n",
    "    first_latents = np.array(first_latents)\n",
    "    last_latents = np.array(last_latents)\n",
    "    return first_latents.mean(0), first_latents.std(0), last_latents.mean(0), last_latents.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0920f247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--fp16'], dest='fp16', nargs=0, const=True, default=False, type=None, choices=None, help='Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--model_type\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
    "parser.add_argument(\"--length\", type=int, default=20)\n",
    "parser.add_argument(\"--stop_token\", type=str, default=None, help=\"Token at which text generation is stopped\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--temperature\",\n",
    "    type=float,\n",
    "    default=1.0,\n",
    "    help=\"temperature of 1.0 has no effect, lower tend toward greedy sampling\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--repetition_penalty\", type=float, default=1.0, help=\"primarily useful for CTRL model; in that case, use 1.2\"\n",
    ")\n",
    "parser.add_argument(\"--k\", type=int, default=0)\n",
    "parser.add_argument(\"--num-sentences\", type=int, default=0)\n",
    "parser.add_argument(\"--split-sentences\", type=int, default=1)\n",
    "parser.add_argument(\"--multiply-sentences\", type=int, default=1)\n",
    "parser.add_argument(\"--p\", type=float, default=0.99)\n",
    "\n",
    "parser.add_argument(\"--prefix\", type=str, default=\"\", help=\"Text added prior to input.\")\n",
    "parser.add_argument(\"--padding_text\", type=str, default=\"\", help=\"Deprecated, the use of `--prefix` is preferred.\")\n",
    "\n",
    "parser.add_argument(\"--no_eos\", action=\"store_false\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\"--dryrun\", action=\"store_true\", default=False, help=\"Text added prior to input.\")\n",
    "parser.add_argument(\"--suppress_eos\", action=\"store_true\", default=False, help=\"Text added prior to input.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--dataset_name\", type=str, default=\"\", help=\"Text added prior to input.\")\n",
    "parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\"--fixed_prompt\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\"--num_return_sequences\", type=int, default=1, help=\"The number of samples to generate.\")\n",
    "parser.add_argument(\"--num_intervals\", type=int, default=1, help=\"The number of samples to generate.\")\n",
    "parser.add_argument(\"--block_size\", type=int, default=1024)\n",
    "parser.add_argument(\"--use_dataset\", action=\"store_true\", default=False, help=\"Text added prior to input.\")\n",
    "parser.add_argument(\"--project\", type=str, default=\"\", help=\"Text added prior to input.\")\n",
    "parser.add_argument(\"--encoder_filepath\", type=str, required=True,default=\"\", help=\"Text added prior to input.\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=3, help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--use_random_embs\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\"--use_true_end_latent\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\"--label\", type=str, default=\"\", help=\"Text added prior to input.\")\n",
    "\n",
    "parser.add_argument(\"--method\", type=str, default=\"\", help=\"Text added prior to input.\")\n",
    "parser.add_argument(\"--first_sentence\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\"--full_section\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\"--autoregressive\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\n",
    "    \"--fp16\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d53740",
   "metadata": {},
   "outputs": [],
   "source": [
    "nseed = 2022\n",
    "path2repo = '/home/sheng136/workspace/myprojects/language_modeling_via_stochastic_processes/'\n",
    "\n",
    "domain = \"tickettalk\"\n",
    "latent_dim = '16'\n",
    "\n",
    "\n",
    "args = parser.parse_args(f\"\"\"--model_type=gpt2 \n",
    "                         --model_name_or_path={path2repo}/language_modeling_via_stochastic_processes/transformers/examples/pytorch/language-modeling/LM_{domain}_{latent_dim}/ \n",
    "                         --prompt=\"<|endoftext|>\" \n",
    "                         --num_return_sequences=1 \n",
    "                         --num_intervals=1000 \n",
    "                         --method=sample \n",
    "                         --stop_token=\"<|endoftext|>\" \n",
    "                         --dataset_name={domain} \n",
    "                         --encoder_filepath={path2repo}/trained_model/{domain}_encoder.ckpt \n",
    "                         --latent_dim={latent_dim} \n",
    "                         --project=LM_{domain} \n",
    "                         --no_eos\n",
    "                         --label=LM_{domain}_{latent_dim} \n",
    "                         --seed={nseed}\"\"\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5cb10a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/16/2022 19:43:40 - WARNING - __main__ -   device: cuda, n_gpu: 1, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
    "args.use_section_null = 0\n",
    "\n",
    "logger.warning(f\"device: {args.device}, n_gpu: {args.n_gpu}, 16-bits training: {args.fp16}\")\n",
    "\n",
    "set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e06648e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "try:\n",
    "    args.model_type = args.model_type.lower()\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "except KeyError:\n",
    "    raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
    "model = model_class.from_pretrained(args.model_name_or_path)\n",
    "model.to(args.device)\n",
    "\n",
    "model.transformer._config.use_contrastive_embeddings = True\n",
    "\n",
    "if args.suppress_eos:\n",
    "    bad_words_ids = [[tokenizer.eos_token_id]]\n",
    "else:\n",
    "    bad_words_ids = None\n",
    "\n",
    "if args.no_eos:\n",
    "    min_length = 1023\n",
    "else:\n",
    "    min_length= 10 # default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425960e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old tokenizer size:  50260\n",
      "Not adding because it's already contained\n",
      "New tokenizer size:  50260\n"
     ]
    }
   ],
   "source": [
    "SECTION_IDS, SPECIAL_TOKENS, tokenizer = get_special_tokens(\n",
    "        dataset_name=args.dataset_name, tokenizer=tokenizer)\n",
    "\n",
    "model.transformer.special_tokens = SPECIAL_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab8c1f",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "863a9f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2OUEncoder(\n",
       "  (model): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (mlp): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "  )\n",
       "  (log_q): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (1): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       "  (C_eta): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = 'gpt2'\n",
    "CL_MODEL = get_checkpoint(\n",
    "        dataset_name=args.dataset_name,\n",
    "        latent_dim=args.latent_dim,\n",
    "        sec_id=True,\n",
    "        token_size=len(tokenizer),\n",
    "        base_model=base_model,\n",
    "        filepath=args.encoder_filepath\n",
    "    )# .to(cpu_device)\n",
    "CL_MODEL.to(args.device)\n",
    "CL_MODEL.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd8a13f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f4af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: Namespace(autoregressive=False, block_size=1024, dataset_name='tickettalk', device=device(type='cuda'), dryrun=False, encoder_filepath='/home/sheng136/workspace/myprojects/language_modeling_via_stochastic_processes//trained_model/tickettalk_encoder.ckpt', first_sentence=False, fixed_prompt=False, fp16=False, full_section=False, k=0, label='LM_tickettalk_16', latent_dim=16, length=20, method='sample', model_name_or_path='/home/sheng136/workspace/myprojects/language_modeling_via_stochastic_processes//language_modeling_via_stochastic_processes/transformers/examples/pytorch/language-modeling/LM_tickettalk_16/', model_type='gpt2', multiply_sentences=1, n_gpu=1, no_cuda=False, no_eos=False, num_intervals=1000, num_return_sequences=1, num_sentences=0, p=0.99, padding_text='', prefix='', project='LM_tickettalk', prompt='\"<|endoftext|>\"', repetition_penalty=1.0, seed=2022, split_sentences=1, stop_token='\"<|endoftext|>\"', suppress_eos=False, temperature=1.0, use_dataset=False, use_random_embs=False, use_section_null=0, use_true_end_latent=False)\n",
      "LOADING MOVIE TM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng136/workspace/myprojects/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/transformers/src/transformers/data/datasets/language_modeling.py:64: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num examples 2376\n",
      "num filtered 0\n",
      "Lengths\n",
      "examples\n",
      "<|endoftext|> [ USER ] hi....am buying a ticket tonight so we go and see a movie at AMC mountain 16 [ ASSISTANT ] No problem. Is there a particular type of movie youâ€™re looking for? [ USER ] hhhmmmmm not at all. i dont have any in mind for now [ ASSISTANT ] Sure. I can help with that. Let me listings at AMC Mercado 24. [ USER ] sure you can but i want to see the movie at AMC mountain 16 [ ASSISTANT ] Oh, sorry about that. So youâ€™re interested in action films at AMC Mountain 16, right? [ USER ] yeah [ ASSISTANT ] OK. I show one action movie playing at AMC Mountain 16: No Time To Die. Remaining showtimes are 4:30pm, 6:40pm and 9:10pm. Does any of those work? [ USER ] yeah but 9.10pm will be perfect for me [ ASSISTANT ] Great. And how many tickets? [ USER ] myself and two other persons are going to see a movie [ ASSISTANT ] All right. Let me confirm that youâ€™d like three tickets for No Time To Die at AMC Mountain 16 tonight at 9:10pm. Is that all correct? [ USER ] yeah [ ASSISTANT ] Is it OK to go ahead and purchase these tickets? [ USER ] yes you can [ ASSISTANT ] OK. Your tickets are purchased and details for how to proceed have been sent to your email address. Can I help with anything else? [ USER ] ok thanks but i dont need anything again [ ASSISTANT ] OK. Enjoy your movie!  <|endoftext|>\n",
      "<|endoftext|> [ USER ] Purchase tickets for Bad Boys for Life, please [ ASSISTANT ] Sure. What time would you like to go? [ USER ] What times are available around 7? [ ASSISTANT ] There are several showtimes around 7:00 PM. There is a 6:45 PM, 7:50 PM, and 8:00 PM. [ USER ] Is there a 7:00 PM showtime? [ ASSISTANT ] No. There is no 7:00 PM showtime? [ USER ] Okay. What about an 7:30 PM showtime? [ ASSISTANT ] There is no 7:30 PM showtime? [ USER ] Are there any 7:00 PM showtimes around Dallas? [ ASSISTANT ] The closest 7:00 PM showtime is 20 miles away. [ USER ] What is the next available showtime for Bad Boys for life or Knives Out? [ ASSISTANT ] There is a 7:50 PM showtime for Bad Boys for life and a 8:30 PM showtime for Knives Out. [ USER ] Book tickets for the 8:30 PM showtime Knives Out. [ ASSISTANT ] How many tickets would you like purchased? [ USER ] Five tickets. [ ASSISTANT ] There are not Five Tickets available for the 8:30 PM showtime of Knives Out. [ USER ] How many tickets are available? [ ASSISTANT ] According to the movie theater app there are only three tickets left for the 8:30 PM showtime of Knives Out. [ USER ] Okay. How many tickets are available for the 7:50 PM showing of Bad Boys for Life? [ ASSISTANT ] There is only one ticket available. [ USER ] Are there any options available within 10 miles? [ ASSISTANT ] There are no options available within 10 miles [ USER ] Okay. Never mind. Do not purchase any tickets.  <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "assert args.dataset_name\n",
    "from language_modeling_via_stochastic_processes.src import constants\n",
    "print(f'Args: {args}')\n",
    "train_path, _, eval_path = get_data_paths(args)\n",
    "train_dataset = get_dataset(\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_path,\n",
    "    special_words=SECTION_IDS,\n",
    "    cache_dir=constants.PATH2HUGGINGFACE,\n",
    "    cl_model=CL_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b89bce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[ USER ]', '[ ASSISTANT ]', ' . ']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.special_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0efed943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING MOVIE TM\n",
      "num examples 1188\n",
      "num filtered 0\n",
      "Lengths\n",
      "examples\n",
      "<|endoftext|> [ USER ] hai sir, we all are like to watch a comedy movie.. we dont like action movie.. i think.. is there is there movie is action movie? [ ASSISTANT ] No problem. Is there a particular type of movie youâ€™re looking for? [ USER ] wow supper... i dont expect this... are you sure sir this movie is a comedy movie? then if are u help me to listinings at amc mercada 24 [ ASSISTANT ] Sure. I can help with that. Let me listings at AMC Mercado 24. [ USER ] sir again i told you... i want watch comedy movie.. is there is the amc mountain 16 is right or wrong? [ ASSISTANT ] Oh, sorry about that. So youâ€™re interested in action films at AMC Mountain 16, right? [ USER ] okay sir.. i was compromise my family to watch action movie now... because i want to see tha amc mountain 16 now [ ASSISTANT ] OK. I show one action movie playing at AMC Mountain 16: No Time To Die. Remaining showtimes are 4:30pm, 6:40pm and 9:10pm. Does any of those work? [ USER ] i have free in 9.10 pm for my busy schedule... so me and my family members are come to your theater in 9.10 pm [ ASSISTANT ] Great. And how many tickets? [ USER ] yeah i want 3 tickets for me and my bro & sis [ ASSISTANT ] All right. Let me confirm that youâ€™d like three tickets for No Time To Die at AMC Mountain 16 tonight at 9:10pm. Is that all correct? [ USER ] mm kk sir.. i dont leave that time [ ASSISTANT ] Is it OK to go ahead and purchase these tickets? [ USER ] mm kk sir.. i will purchase the 3 tickets.. [ ASSISTANT ] OK. Your tickets are purchased and details for how to proceed have been sent to your email address. Can I help with anything else? [ USER ] still your helping is very thank ful sir... then i will manage it.. u dont help me.. [ ASSISTANT ] OK. Enjoy your movie!  <|endoftext|>\n",
      "<|endoftext|> [ USER ] Hello! I would like to get some movie tickets. [ ASSISTANT ] Good morning! What movie would you like me to book tickets for? [ USER ] I would like to watch Uncut Gems. [ ASSISTANT ] Great! What's the address or name of the theater where you want to watch Uncut Gems? [ USER ] The AMC on Cypress. It's the only theater in my area. [ ASSISTANT ] Awesome, what day would you like the tickets for? [ USER ] This Friday. [ ASSISTANT ] The AMC at 1600 Calhoun Rd, Cypress, TX, has the following showtimes for Uncut Gems: 12:00 PM, 01:00 PM and 09:30 PM. [ USER ] None of those work for me. Isn't there a showtime at 05:00 PM or 02:00 PM. [ ASSISTANT ] The theater you selected doesn't have those showtimes. I'm sorry. Would you like to see showtimes on other theaters or for other movies? [ USER ] Are there any showings of Doctor Doolittle in that same AMC? [ ASSISTANT ] For this Friday as well? [ USER ] Yes, at 05:00 PM or 02:00 PM. [ ASSISTANT ] There is a 02:00 PM show for Doctor Doolittle. [ USER ] Awesome! [ ASSISTANT ] How many tickets would you like me to book? [ USER ] Six. [ ASSISTANT ] I'm sorry, there are only four seats available for the 02:00 PM show. Would you like me to look at the Saturday shows instead? [ USER ] No, it's ok. I'll see if my friends want to watch something else. [ ASSISTANT ] Sorry for the inconvenience!  <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = get_dataset(\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=eval_path,\n",
    "        special_words=SECTION_IDS,\n",
    "        cache_dir=constants.PATH2HUGGINGFACE,\n",
    "        cl_model=CL_MODEL,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02c6f625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376\n"
     ]
    }
   ],
   "source": [
    "# Estimate density for last sentence\n",
    "# Average density for all training samples\n",
    "first_latent_mu, first_latent_std, last_latent_mu, last_latent_std = get_density(dataset=train_dataset, lm=model, cl_model=CL_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4abafb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12817074,  0.11731824,  0.08505533, -0.12896992,  0.13096477,\n",
       "        0.12195772,  0.12528765, -0.10445517,  0.11606154,  0.13890566,\n",
       "       -0.13555746,  0.15454775, -0.13034433,  0.12845673,  0.14243399,\n",
       "        0.13648628], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_latent_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bde6a696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20150964, 0.19876935, 0.1916553 , 0.1879684 , 0.19223699,\n",
       "       0.19855234, 0.19538742, 0.19835728, 0.2010545 , 0.19634782,\n",
       "       0.19453451, 0.19091173, 0.1909538 , 0.18812436, 0.19124323,\n",
       "       0.19830342], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_latent_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f0a7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_intervals = len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0794c477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1188"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15dbb041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last latent mu [-0.52328944  0.5134945   0.5024361  -0.508544    0.5275813   0.5180528\n",
      "  0.5092071  -0.51341796  0.5401892   0.52739847 -0.5112348   0.53281057\n",
      " -0.51549774  0.5339563   0.5201655   0.5258357 ]\n",
      "last latent std [0.20150964 0.19876935 0.1916553  0.1879684  0.19223699 0.19855234\n",
      " 0.19538742 0.19835728 0.2010545  0.19634782 0.19453451 0.19091173\n",
      " 0.1909538  0.18812436 0.19124323 0.19830342]\n"
     ]
    }
   ],
   "source": [
    "print(\"last latent mu\", last_latent_mu)\n",
    "print(\"last latent std\", last_latent_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66e3d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking example embeddings: tensor([-0.1766,  0.1473,  0.1222, -0.1754,  0.1808,  0.1625,  0.1590, -0.1451,\n",
      "         0.1540,  0.1787, -0.1800,  0.1995, -0.1605,  0.1790,  0.1835,  0.1909],\n",
      "       device='cuda:0')\n",
      "Checking example embeddings: tensor([-0.7280,  0.7133,  0.6992, -0.6989,  0.7188,  0.7198,  0.7100, -0.7148,\n",
      "         0.7379,  0.7346, -0.7065,  0.7187, -0.7052,  0.7223,  0.7213,  0.7226],\n",
      "       device='cuda:0')\n",
      "Checking example embeddings: tensor([-0.1025,  0.0956,  0.0594, -0.1013,  0.0859,  0.0948,  0.1046, -0.0729,\n",
      "         0.0904,  0.1108, -0.1112,  0.1333, -0.1023,  0.0926,  0.1122,  0.0967],\n",
      "       device='cuda:0')\n",
      "Checking example embeddings: tensor([-0.2416,  0.2438,  0.2398, -0.2420,  0.2593,  0.2482,  0.2493, -0.2351,\n",
      "         0.2556,  0.2598, -0.2406,  0.2530, -0.2498,  0.2663,  0.2442,  0.2473],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking example embeddings: {}\".format(eval_dataset.cl_embeddings[0][0]))\n",
    "print(\"Checking example embeddings: {}\".format(eval_dataset.cl_embeddings[0][-1]))\n",
    "print(\"Checking example embeddings: {}\".format(eval_dataset.cl_embeddings[-1][0]))\n",
    "print(\"Checking example embeddings: {}\".format(eval_dataset.cl_embeddings[-1][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba1c3e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[ USER ]', '[ ASSISTANT ]', ' . ']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.special_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a060da1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset.cl_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2252533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.cl_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b856c558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[ USER ] I would like to buy three tickets for tonight. [ ASSISTANT ] No problem. Is there a particular type of movie youâ€™re looking for? [ USER ] My two buds and I are going. Probably an action movie. Nothing mushy! [ ASSISTANT ] Sure. I can help with that. Let me check listings at AMC Mercado 24. [ USER ] Oh, Pardon me. We wanted to go to the AMC Mountain 16. [ ASSISTANT ] Oh, sorry about that. So youâ€™re interested in action films at AMC Mountain 16, right? [ USER ] You are correct! [ ASSISTANT ] OK. I see one action movie playing at AMC Mountain 16: No Time To Die. Remaining showtimes are 4:30pm, 6:40pm and 9:10pm. Does any of those work? [ USER ] That sounds good. We'd probably want to go to the 9:10pm showing. [ ASSISTANT ] Great. And how many tickets? [ USER ] My friends Joe, Bob, and I are going to the movie. So, 3. [ ASSISTANT ] All right. Let me confirm that youâ€™d like three tickets for No Time To Die at AMC Mountain 16 tonight at 9:10pm. Is that all correct? [ USER ] Yes, that is correct! [ ASSISTANT ] Is it OK to go ahead and purchase these tickets? [ USER ] Yes, please! [ ASSISTANT ] OK. Your tickets are purchased and details for how to proceed have been sent to your email address. Can I help with anything else? [ USER ] No. Thank you. You have been very helpful. [ ASSISTANT ] OK. Enjoy your movie! \""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.cl_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb6b0ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|endoftext|> [ USER ] I would like to buy three tickets for tonight. [ ASSISTANT ] No problem. Is there a particular type of movie youâ€™re looking for? [ USER ] My two buds and I are going. Probably an action movie. Nothing mushy! [ ASSISTANT ] Sure. I can help with that. Let me check listings at AMC Mercado 24. [ USER ] Oh, Pardon me. We wanted to go to the AMC Mountain 16. [ ASSISTANT ] Oh, sorry about that. So youâ€™re interested in action films at AMC Mountain 16, right? [ USER ] You are correct! [ ASSISTANT ] OK. I see one action movie playing at AMC Mountain 16: No Time To Die. Remaining showtimes are 4:30pm, 6:40pm and 9:10pm. Does any of those work? [ USER ] That sounds good. We'd probably want to go to the 9:10pm showing. [ ASSISTANT ] Great. And how many tickets? [ USER ] My friends Joe, Bob, and I are going to the movie. So, 3. [ ASSISTANT ] All right. Let me confirm that youâ€™d like three tickets for No Time To Die at AMC Mountain 16 tonight at 9:10pm. Is that all correct? [ USER ] Yes, that is correct! [ ASSISTANT ] Is it OK to go ahead and purchase these tickets? [ USER ] Yes, please! [ ASSISTANT ] OK. Your tickets are purchased and details for how to proceed have been sent to your email address. Can I help with anything else? [ USER ] No. Thank you. You have been very helpful. [ ASSISTANT ] OK. Enjoy your movie!  <|endoftext|>\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.raw_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6d65ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_example = random.sample(range(num_intervals), k = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97faee27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c05926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1188\n",
    "row = eval_dataset.cl_texts[num_example]\n",
    "row = row.replace('<newline>', '')\n",
    "row = row.replace(' , ', ', ')\n",
    "row = row.strip() # NOTE: remove break line\n",
    "row = ' '.join(row.split()) # remove multiple spaces\n",
    "split_pattern = \" . \"\n",
    "split_text = row.split(split_pattern)[:-1]\n",
    "split_text = [ _ + split_pattern for _ in split_text ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f53c6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c97df8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cl_feats = torch.stack(eval_dataset.cl_embeddings[num_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfdb5f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([339, 16])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_cl_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35d464b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cl_feats = true_cl_feats[::args.split_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0d5ea975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENSITY ESTIMATE: [-0.52328944  0.5134945   0.5024361  -0.508544    0.5275813   0.5180528\n",
      "  0.5092071  -0.51341796  0.5401892   0.52739847 -0.5112348   0.53281057\n",
      " -0.51549774  0.5339563   0.5201655   0.5258357 ]\n",
      "DENSITY ESTIMATE STD: [0.20150964 0.19876935 0.1916553  0.1879684  0.19223699 0.19855234\n",
      " 0.19538742 0.19835728 0.2010545  0.19634782 0.19453451 0.19091173\n",
      " 0.1909538  0.18812436 0.19124323 0.19830342]\n"
     ]
    }
   ],
   "source": [
    "LABELS = ['TRUE CL', 'BRIDGE CL (DE)','RANDOM CL'\n",
    "]\n",
    "# INTERPOLATION - BRIDGE\n",
    "print(f\"DENSITY ESTIMATE: {last_latent_mu}\")\n",
    "print(f\"DENSITY ESTIMATE STD: {last_latent_std}\")\n",
    "B_T = np.random.normal(loc=last_latent_mu, scale=last_latent_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2802486e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_cl_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4cbad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = len(true_cl_feats) if not args.split_sentences else int(len(true_cl_feats)/float(args.split_sentences))\n",
    "num_sentences *= args.multiply_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af12326c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "345eead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_inputs = eval_dataset.examples[num_example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a8e2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = eval_dataset.get_end_points(actual_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11621bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbe58e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_sentences = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77cab19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original num sentences: 23\n",
      "Target num sentences: 40\n",
      "min length 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Original num sentences: {}\".format(len(end)))\n",
    "print(\"Target num sentences: {}\".format(num_sentences))\n",
    "print(\"min length\", min_length)\n",
    "end_lengths = [end[i] if i == 0 else end[i+1] - end[i] for i in range(len(end)-1)]\n",
    "end_lengths = (np.array(end_lengths)*(num_sentences/len(end)))\n",
    "#end_lengths = np.ones(end_lengths.shape)\n",
    "end_lengths = end_lengths.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20f31976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 19, 29, 19, 27,  5, 22, 26, 66, 19, 31, 17, 12, 15, 26, 62,  5,\n",
       "       20,  5, 48, 10, 15])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0c0347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(train_dataset.cl_embeddings[0])):\n",
    "    X[row] = train_dataset.cl_embeddings[0][row].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb2cdadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 19, 29, 19, 27,  5, 22, 26, 66, 19, 31, 17, 12, 15, 26, 62,  5,\n",
       "       20,  5, 48, 10, 15])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "576d281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bridge_feats = simulate_brownian_bridge(B_0=true_cl_feats[0], B_T=B_T, num_samples=num_sentences,sentence_lengths=end_lengths)\n",
    "bridge_feats = simulate_brownian_bridge(\n",
    "                B_0=true_cl_feats[0], B_T=B_T, num_samples=num_sentences,\n",
    "                sentence_lengths=end_lengths\n",
    "            )\n",
    "\n",
    "bridge_feats = torch.tensor(bridge_feats, dtype=true_cl_feats.dtype).to(args.device)\n",
    "\n",
    "# RANDOM\n",
    "random_feats = torch.rand(true_cl_feats.shape).to(args.device)\n",
    "feats = [true_cl_feats, bridge_feats, random_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2849f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generation_metrics import GenerationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d68a2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = args.model_name_or_path.split('/')[-2]\n",
    "args.encoder_type = 'contrast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9583c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LM_tickettalk_16'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ed0b17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_cl_tracker = GenerationMetrics(model=model, device=args.device,\n",
    "                                tokenizer=tokenizer, dataset_name=args.dataset_name,\n",
    "                                fname=fname+\"_trueCLEmbs_\" + args.method,\n",
    "                                model_args=args,\n",
    "                                subclass=\"GT\")\n",
    "random_cl_tracker = GenerationMetrics(model=model, device=args.device,\n",
    "                                tokenizer=tokenizer, dataset_name=args.dataset_name,\n",
    "                            model_args=args,\n",
    "                                fname=fname+\"_randomCLEmbs_\"+args.method,\n",
    "                             subclass=\"RANDOM\")\n",
    "bridge_cl_tracker = GenerationMetrics(model=model, device=args.device,\n",
    "                                tokenizer=tokenizer, dataset_name=args.dataset_name,\n",
    "                                fname=fname+\"_bridgeCLEmbs_\"+args.method,\n",
    "                            model_args=args,\n",
    "                                subclass=\"BRIDGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6d6fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = args.prompt if args.prompt else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3c49c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "trackers = [gt_cl_tracker, bridge_cl_tracker,\n",
    "            random_cl_tracker\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb55654e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"<|endoftext|>\"'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ae24f3ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated length: 316\n",
      "[ GENERATED FOR TRUE CL ]: \"<|endoftext|>\"Do you know what movie tickets are available for tonight for Sure Place in the Mountain AMC 16 Cinemark downtown?\"  [ USER ]  Yes, sir. I would like to buy tickets to The Call of the Wild, which is playing at that theater.  [ ASSISTANT ]  Oh, wait, wait a second. I was referring to a new release, is this already done?  [ USER ]  Yes, then. Thank you.  [ ASSISTANT ]  Now, would you like me to fetch these tickets?  [ USER ]  Yes, please.  [ ASSISTANT ]  Okay. Correct?  [ USER ]  Sure.  [ ASSISTANT ]  To confirm, you would like to purchase one ticket for today's 6:20 PM showing of The Call of the Wild at the Mountain AMC 16 Cinemark downtown?  [ USER ]  Yes, please.  [ ASSISTANT ]  Okay? Hmmm. So about what time would you like to hear the movie?  [ USER ]  I would like to hear the show time at 12:00 PM for two.  [ ASSISTANT ]  Okay. How many tickets would you like to purchase?  [ USER ]  Three tickets. Just one ticket.  [ ASSISTANT ]  Purchasing now.  [ USER ]  Yes.  [ ASSISTANT ]  Of course. Yes, sir. I would like to purchase three tickets to the check-tickets showing of The Call of the Wild tonight at 12:00 PM at the Mountain AMC 16 Cinemark downtown.  [ USER ]  Yes. And, please make payment at the same time.  [ ASSISTANT ]  Okay. I am processing your order. Your order number is hd88298824.  [ USER ]  Thank you.  [ ASSISTANT ]  You are most welcome. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated length: 205\n",
      "[ GENERATED FOR BRIDGE CL (DE) ]: \"<|endoftext|>\"Esquire Petite  [ ASSISTANT ]  Hello!  [ USER ]  And can I get three tickets for what kind of movie is playing currently?  [ ASSISTANT ]  There are 3 movies playing currently. The first movie is Action which is rated PG 13. The second movie is Rotten Tomatoes which is rated R. And finally you have another movie called Horror which is rated R.  [ USER ]  Which one is Rotten Tomatoes?  [ ASSISTANT ]  Horror is rated R with a Rotten Tomatoes score of 61%  [ USER ]  Which one is Rotten Tomatoes, IRL, and which one is Rotten Tomatoes, I think?  [ ASSISTANT ]  This theater currently has 2 Action movies playing. The first is titled, \"The Turning\" and starts at 1pm, 3pm, 6pm and 8pm. The second is titled \"The Turning\" and starts at 1pm, 3pm, 6pm and 8pm.  [ USER ]  Thank you so much!  [ ASSISTANT ]  You're welcome. Go ahead and order them. \n",
      "Generated length: 359\n",
      "[ GENERATED FOR RANDOM CL ]: \"<|endoftext|>\"Are there any theaters in Spring Hills Kansas right now that is playing No Time To Die?  [ ASSISTANT ]  Yes, that is currently playing at the AMC Holiday Theater.  [ USER ]  I would like to go and see that movie please.  [ ASSISTANT ]  Sure, what kind of movie do you prefer to see?  [ USER ]  I want to see a good movie that I am not afraid to watch. Do you have any Stephen King movies?  [ ASSISTANT ]  Yes, there is also an abc king in town  [ USER ]  Oh, wow, that is quite good.  [ ASSISTANT ]  You have a great day, do you have a screening preference?  [ USER ]  No, I prefer to go to the at the earliest time.  [ ASSISTANT ]  Okay, this movie is playing at Cima Ultraplex 20 at 4:00pm 6:10pm 8:30pm 10:40pm  [ USER ]  That is better.  [ ASSISTANT ]  You would like to see it at the earliest time, times also available at the AMC Holiday Theater are 3:00pm 5:10pm 7:30pm and 9:40pm  [ USER ]  So much more information would be helpful.  [ ASSISTANT ]  Absolutely, here are the options available. Which has the higher scores for you?  [ USER ]  The better scores the better, so then I prefer No Time To Die.  [ ASSISTANT ]  Standard are always available there but have less for non-Drama or comedy.  [ USER ]  That is fine if I buy the tickets well and keep my party happy.  [ ASSISTANT ]  There are no more discounts available for these either. Which one do you prefer?  [ USER ]  Well, for the comedy part please, I prefer to skip it and go to the family movie theater.  [ ASSISTANT ]  At the cost of three tickets, no worries, you can enjoy a movie. \n"
     ]
    }
   ],
   "source": [
    "for seq_i, (seq_cl_feats, tracker) in enumerate(zip(feats, trackers)):\n",
    "    cl_feats = seq_cl_feats[0] # Get the first sentence feat\n",
    "    prefix = args.prefix if args.prefix else args.padding_text\n",
    "    encoded_prompt = tokenizer.encode(prefix + prompt_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    encoded_prompt = encoded_prompt.to(args.device)\n",
    "\n",
    "    if encoded_prompt.size()[-1] == 0:\n",
    "        input_ids = None\n",
    "    else:\n",
    "        input_ids = encoded_prompt\n",
    "\n",
    "    if 'filter' in args.dataset_name:\n",
    "        length = 1024\n",
    "    else:\n",
    "        length = 1024 # len(eval_dataset.examples[_])\n",
    "\n",
    "            # RESET THE CL INDEX\n",
    "    model.transformer._cur_cl_idx = 0\n",
    "    model.transformer._has_reset = False\n",
    "\n",
    "    max_length = min(length + len(encoded_prompt[0]), 1024)\n",
    "    if args.no_eos:\n",
    "        max_length = 1024\n",
    "\n",
    "    if args.method == \"sample\":\n",
    "        output_sequences = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    section_ids=None,\n",
    "                    cl_feats=cl_feats, # .to(args.device),\n",
    "                    seq_cl_feats=seq_cl_feats,\n",
    "                    max_length=max_length,\n",
    "                    temperature=args.temperature,\n",
    "                    top_k=args.k,\n",
    "                    top_p=args.p,\n",
    "                    repetition_penalty=args.repetition_penalty,\n",
    "                    do_sample=True,\n",
    "                    num_return_sequences=args.num_return_sequences,\n",
    "                    bad_words_ids=bad_words_ids,\n",
    "                    min_length=min_length-50\n",
    "                )\n",
    "\n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_()\n",
    "\n",
    "    generated_sequences = []\n",
    "    \n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    # print(f\"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===\")\n",
    "        original = torch.clone(generated_sequence)\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "        print(\"Generated length: {}\".format(len(generated_sequence)))\n",
    "\n",
    "        # Decode text\n",
    "        # text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "\n",
    "        # Remove all text after the stop token\n",
    "        text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "\n",
    "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "        total_sequence = (\n",
    "        prompt_text + text[len(tokenizer.decode(encoded_prompt[0], skip_special_tokens=True)) :]\n",
    "           )\n",
    "\n",
    "        gt_raw_seq = eval_dataset.raw_texts[num_example]\n",
    "        generated_sequences.append(total_sequence)\n",
    "        print(\"[ GENERATED FOR {} ]: {}\".format(LABELS[seq_i], total_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38dc88ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|endoftext|> [ USER ] hi i'd like to buy tickets tonight for the amc mountain 16 please. [ ASSISTANT ] OK. I understand that youâ€™d like to get tickets for tonight. Can you tell me the city and state where the theater is located? [ USER ] it's california, santa barbara [ ASSISTANT ] OK. And are you interested in AMC Mountain 16 or AMC Mercado 24? [ USER ] i already said i wanted amc mountain 16. [ ASSISTANT ] Sorry for the trouble but I wanted to make sure I have it right. [ USER ] OK. [ ASSISTANT ] Is there a particular movie youâ€™re looking for? [ USER ] can we skip to the end please? i'm in a hurry. [ ASSISTANT ] OK, so far I understand that you are interested in tickets for tonight at AMC Mountain 16. We just need to decide on a movie, the number of tickets and a showtime. [ USER ] what is playing at amc mountain 16 please. [ ASSISTANT ] I found two movies at AMC Mountain 16: No Time To Die and Mulan. [ USER ] tickets for no time to die please. [ USER ] i prefer 9:10pm [ ASSISTANT ] Got it. And how many tickets? [ USER ] tickets for me tanya, dave, and cree please [ ASSISTANT ] All right. Let me confirm that youâ€™d like three tickets for No Time To Die at AMC Mountain 16 tonight at 9:10pm. Is that all correct? [ USER ] yep [ ASSISTANT ] Is it OK to go ahead and purchase these tickets? [ USER ] yep [ ASSISTANT ] OK. Your tickets are purchased and details for how to proceed have been sent to your email address. Can I help with anything else? [ USER ] nope thats all thanks [ ASSISTANT ] OK. Enjoy your movie!  <|endoftext|>\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.raw_texts[num_example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813d1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7753e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmvs",
   "language": "python",
   "name": "lmvs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
